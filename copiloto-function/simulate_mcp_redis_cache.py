# =======================================================================
# SIMULADOR DEL ENTORNO MCP CON CACHE REDIS
# =======================================================================
# Este script simula el comportamiento del servidor MCP usando directamente
# el redis_buffer_service para demostrar el cache hit/miss

import os
import sys
import time
import logging
from typing import Dict, Any

# Configurar el path para importar los servicios
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Configurar logging detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

# Importar servicios
try:
    from services.redis_buffer_service import redis_buffer
    from openai import AzureOpenAI
    print("âœ… Servicios importados correctamente")
except ImportError as e:
    print(f"âŒ Error importando servicios: {e}")
    sys.exit(1)

# ConfiguraciÃ³n
DEFAULT_MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o-mini")
TEST_AGENT_ID = "MCP_Test_Agent"
TEST_SESSION_ID = "mcp_simulation_session"


def get_openai_client():
    """Obtener cliente OpenAI configurado"""
    try:
        azure_openai_key = os.environ.get("AZURE_OPENAI_KEY")
        azure_openai_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")

        if not azure_openai_key or not azure_openai_endpoint:
            raise ValueError(
                "AZURE_OPENAI_KEY and AZURE_OPENAI_ENDPOINT deben estar configurados")

        return AzureOpenAI(
            api_key=azure_openai_key,
            api_version="2024-02-01",
            azure_endpoint=azure_openai_endpoint,
        )
    except Exception as e:
        logging.error(f"âŒ Error configurando OpenAI client: {e}")
        return None


def simulate_mcp_redis_cache(mensaje: str, agent_id: str = TEST_AGENT_ID, session_id: str = TEST_SESSION_ID, model: str = DEFAULT_MODEL) -> Dict[str, Any]:
    """Simula el comportamiento del MCP con cache Redis directo"""

    logging.info("=" * 80)
    logging.info(f"ğŸš€ SIMULANDO MCP REQUEST")
    logging.info("=" * 80)

    start = time.perf_counter()
    cache_hit = False
    origen = "model"
    cache_source = "miss"
    respuesta_texto = None

    # Generar hash del mensaje para logging
    msg_hash = redis_buffer.stable_hash(mensaje)
    logging.info(f"ğŸ“¥ Request: agent={agent_id}, session={session_id}")
    logging.info(f"ğŸ“ Message: {mensaje}")
    logging.info(f"ğŸ”¢ Message hash: {msg_hash}")
    logging.info(f"ğŸ¤– Model: {model}")
    logging.info("")

    # Verificar estado de Redis
    logging.info(f"ğŸ” Redis Status: enabled={redis_buffer.is_enabled}")
    if not redis_buffer.is_enabled:
        logging.warning("âš ï¸ Redis estÃ¡ deshabilitado!")
        return {"ok": False, "error": "Redis cache deshabilitado"}

    # Construir claves para logging detallado
    session_key = redis_buffer.build_llm_session_key(
        agent_id, session_id, mensaje, model)
    global_key = redis_buffer.build_llm_global_key(agent_id, mensaje, model)

    logging.info(f"ğŸ”‘ Session key: {session_key}")
    logging.info(f"ğŸŒ Global key:  {global_key}")
    logging.info("")

    # Intentar obtener desde cache Redis
    logging.info("ğŸ” Buscando en Redis cache...")

    cached, cache_source = redis_buffer.get_llm_cached_response(
        agent_id=agent_id,
        session_id=session_id,
        message=mensaje,
        model=model,
        use_global_cache=True,
    )

    if cached:
        cache_hit = True
        origen = f"redis_{cache_source}"
        respuesta_texto = cached.get(
            "respuesta") if isinstance(cached, dict) else cached
        logging.info(f"âœ… CACHE HIT! Source: {cache_source}")
        logging.info(
            f"ğŸ“¤ Cached response preview: {str(respuesta_texto)[:100]}...")
        logging.info("")
    else:
        logging.info(f"âŒ CACHE MISS - calling OpenAI model")
        logging.info("")

        # Cache miss: llamar al modelo
        try:
            logging.info(f"ğŸ¤– Calling OpenAI model: {model}")
            openai_client = get_openai_client()
            if not openai_client:
                return {"ok": False, "error": "No se pudo configurar OpenAI client"}

            completion = openai_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": mensaje}],
            )
            respuesta_texto = completion.choices[0].message.content
            logging.info(
                f"âœ… Model response received: {len(respuesta_texto)} chars")
            logging.info(f"ğŸ“¤ Response preview: {respuesta_texto[:100]}...")
            logging.info("")

            # Guardar en cache
            try:
                logging.info("ğŸ’¾ Saving to Redis cache...")
                redis_buffer.cache_llm_response(
                    agent_id=agent_id,
                    session_id=session_id,
                    message=mensaje,
                    model=model,
                    response_data={
                        "respuesta": respuesta_texto,
                        "session_id": session_id,
                        "agent_id": agent_id,
                        "model": model,
                        "timestamp": time.time()
                    },
                    use_global_cache=True,
                )
                logging.info("âœ… Response cached successfully")
                logging.info("")
            except Exception as cache_err:
                logging.error(f"âŒ Cache write error: {cache_err}")

        except Exception as e:
            logging.error(f"âŒ OpenAI model error: {e}")
            return {"ok": False, "error": f"Error llamando al modelo: {e}"}

    dur_ms = (time.perf_counter() - start) * 1000

    # Resumen final
    logging.info("ğŸ“Š SUMMARY:")
    logging.info(f"   Cache Hit: {cache_hit}")
    logging.info(f"   Source: {cache_source}")
    logging.info(f"   Duration: {dur_ms:.0f}ms")
    logging.info(f"   Response Length: {len(str(respuesta_texto))} chars")

    return {
        "ok": True,
        "respuesta": respuesta_texto,
        "cache_hit": cache_hit,
        "origen": origen,
        "duration_ms": round(dur_ms, 2),
        "session_id": session_id,
        "agent_id": agent_id,
        "model": model,
        "cache_source": cache_source,
        "redis_enabled": redis_buffer.is_enabled,
        "msg_hash": msg_hash[:16],
        "session_key": session_key,
        "global_key": global_key
    }


def run_cache_test():
    """Ejecutar prueba completa de cache hit/miss"""

    print("\nğŸ¯ SIMULADOR MCP - PRUEBA DE CACHE REDIS")
    print("=" * 60)
    print()

    # Mensaje de prueba
    test_message = "Â¿CÃ³mo funciona el motor fuera de borda de una lancha pequeÃ±a?"

    print(f"ğŸ“ Mensaje de prueba: {test_message}")
    print()

    # Primera llamada (deberÃ­a ser cache miss)
    print("ğŸ”„ PRIMERA LLAMADA (esperamos CACHE MISS)")
    print("-" * 50)
    result1 = simulate_mcp_redis_cache(test_message)

    if not result1["ok"]:
        print(f"âŒ Error en primera llamada: {result1['error']}")
        return

    print(f"âœ… Primera llamada completada:")
    print(f"   Cache Hit: {result1['cache_hit']}")
    print(f"   Duration: {result1['duration_ms']}ms")
    print(f"   Source: {result1['cache_source']}")
    print()

    # Esperar un momento
    print("â³ Esperando 2 segundos...")
    time.sleep(2)
    print()

    # Segunda llamada (deberÃ­a ser cache hit)
    print("ğŸ”„ SEGUNDA LLAMADA (esperamos CACHE HIT)")
    print("-" * 50)
    result2 = simulate_mcp_redis_cache(test_message)

    if not result2["ok"]:
        print(f"âŒ Error en segunda llamada: {result2['error']}")
        return

    print(f"âœ… Segunda llamada completada:")
    print(f"   Cache Hit: {result2['cache_hit']}")
    print(f"   Duration: {result2['duration_ms']}ms")
    print(f"   Source: {result2['cache_source']}")
    print()

    # Verificar que la segunda fue cache hit
    if result2['cache_hit']:
        print("ğŸ‰ Â¡Ã‰XITO! Cache funcionando correctamente:")
        print(f"   âœ… Primera llamada: MISS ({result1['duration_ms']}ms)")
        print(f"   âœ… Segunda llamada: HIT ({result2['duration_ms']}ms)")
        print(
            f"   ğŸš€ Speedup: {result1['duration_ms']/result2['duration_ms']:.1f}x mÃ¡s rÃ¡pido")
    else:
        print("âš ï¸ PROBLEMA: Segunda llamada no fue cache hit")
        print("   Posibles causas:")
        print("   â€¢ TTL muy corto")
        print("   â€¢ Error en cache write")
        print("   â€¢ Hash diferente")

    print()
    print("ğŸ” CLAVES GENERADAS:")
    print(f"   Session: {result1['session_key']}")
    print(f"   Global:  {result1['global_key']}")
    print(f"   Hash:    {result1['msg_hash']}")


def test_multiple_messages():
    """Probar con mÃºltiples mensajes diferentes"""

    print("\nğŸ§ª PRUEBA CON MÃšLTIPLES MENSAJES")
    print("=" * 60)

    test_messages = [
        "Â¿QuÃ© es un barco?",
        "Explica los tipos de embarcaciones",
        "Â¿CÃ³mo funciona un motor de lancha?",
        "Â¿QuÃ© es un barco?"  # Repetido para cache hit
    ]

    results = []

    for i, msg in enumerate(test_messages, 1):
        print(f"\nğŸ“ Mensaje {i}: {msg}")
        print("-" * 40)

        result = simulate_mcp_redis_cache(
            msg, agent_id=f"TestAgent_{i%2}", session_id=f"session_{i%2}")
        results.append(result)

        if result["ok"]:
            status = "HIT" if result["cache_hit"] else "MISS"
            print(
                f"   {status} - {result['duration_ms']}ms - {result['cache_source']}")
        else:
            print(f"   ERROR: {result['error']}")

        time.sleep(1)

    print(f"\nğŸ“Š RESUMEN DE {len(results)} LLAMADAS:")
    hits = sum(1 for r in results if r.get("cache_hit", False))
    misses = len(results) - hits
    print(f"   Cache Hits: {hits}")
    print(f"   Cache Misses: {misses}")
    print(f"   Hit Ratio: {hits/len(results)*100:.1f}%")


if __name__ == "__main__":
    try:
        print("ğŸš€ Iniciando simulador MCP con Redis cache...")
        print()

        # Verificar configuraciÃ³n
        print("ğŸ”§ Verificando configuraciÃ³n...")
        print(f"   Redis enabled: {redis_buffer.is_enabled}")
        print(
            f"   OpenAI endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT', 'Not configured')}")
        print(f"   Model: {DEFAULT_MODEL}")
        print()

        if not redis_buffer.is_enabled:
            print("âŒ Redis no estÃ¡ habilitado. Verifica la configuraciÃ³n.")
            sys.exit(1)

        # Ejecutar pruebas
        run_cache_test()
        test_multiple_messages()

        print("\nğŸ¯ SimulaciÃ³n completada exitosamente!")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ SimulaciÃ³n interrumpida por el usuario")
    except Exception as e:
        print(f"\nâŒ Error en simulaciÃ³n: {e}")
        logging.exception("Error completo:")
