#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test para identificar campos necesarios en buscar-memoria 
para respuestas semÃ¡nticas coherentes del LLM
"""

import json
import requests
import time
from typing import Dict, Any, List

# ConfiguraciÃ³n
BASE_URL = "http://localhost:7071"
ENDPOINT = "/api/buscar-memoria"


def hacer_request(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Hacer request al endpoint y manejar respuesta"""
    try:
        print(f"\nğŸ”„ Enviando: {json.dumps(payload, indent=2)}")
        response = requests.post(
            f"{BASE_URL}{ENDPOINT}",
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=30
        )

        print(f"ğŸ“Š Status Code: {response.status_code}")

        try:
            result = response.json()
        except:
            result = {"error": "No JSON response", "text": response.text[:500]}

        return {
            "status_code": response.status_code,
            "payload_enviado": payload,
            "response": result,
            "headers": dict(response.headers)
        }

    except Exception as e:
        return {
            "status_code": 0,
            "payload_enviado": payload,
            "response": {"error": f"Request failed: {str(e)}"},
            "headers": {}
        }


def analizar_calidad_respuesta(response_data: Dict[str, Any]) -> Dict[str, Any]:
    """Analizar quÃ© tan buena es la respuesta para un LLM"""

    if not response_data.get("response", {}).get("exito"):
        return {
            "calidad": "ERROR",
            "puntuacion": 0,
            "problemas": ["Request fallÃ³"],
            "campos_utiles": [],
            "narrativa_posible": False,
            "total_documentos": 0,
            "contexto_rico": 0
        }

    resp = response_data["response"]
    problemas = []
    campos_utiles = []
    puntuacion = 0

    # 1. Â¿Hay documentos?
    documentos = resp.get("documentos", [])
    if not documentos:
        problemas.append("Sin documentos recuperados")
    else:
        puntuacion += 20
        campos_utiles.append("documentos")

    # 2. Â¿Los documentos tienen contexto narrativo?
    contexto_rico = 0
    for doc in documentos[:3]:  # Revisar primeros 3
        if doc.get("texto_semantico") and len(doc.get("texto_semantico", "")) > 10:
            contexto_rico += 1
        if doc.get("contexto_conversacional"):
            contexto_rico += 1
        if doc.get("respuesta_usuario"):
            contexto_rico += 1
        if doc.get("temas_principales"):
            contexto_rico += 1

    if contexto_rico > 0:
        puntuacion += min(30, contexto_rico * 5)
        campos_utiles.extend(
            ["texto_semantico", "contexto_conversacional", "respuesta_usuario"])
    else:
        problemas.append("Documentos sin contexto conversacional rico")

    # 3. Â¿Hay metadatos temporales y de sesiÃ³n?
    metadata = resp.get("metadata", {})
    if metadata.get("session_id") and metadata.get("agent_id"):
        puntuacion += 15
        campos_utiles.extend(["session_id", "agent_id"])
    else:
        problemas.append("Sin identificadores de sesiÃ³n coherentes")

    # 4. Â¿Hay resumen o contexto inteligente?
    if resp.get("contexto_inteligente", {}).get("resumen"):
        puntuacion += 20
        campos_utiles.append("contexto_inteligente")
    elif resp.get("resumen_conversacion"):
        puntuacion += 15
        campos_utiles.append("resumen_conversacion")
    else:
        problemas.append("Sin resumen o contexto inteligente")

    # 5. Â¿Hay informaciÃ³n de continuidad?
    if resp.get("detalles_operacion", {}).get("total", 0) > 0:
        puntuacion += 10
        campos_utiles.append("detalles_operacion")

    # 6. Â¿Los timestamps permiten narrativa temporal?
    timestamps_validos = 0
    for doc in documentos[:5]:
        if doc.get("timestamp"):
            timestamps_validos += 1

    if timestamps_validos > 2:
        puntuacion += 5
        campos_utiles.append("timestamp")

    # Determinar calidad general
    if puntuacion >= 70:
        calidad = "EXCELENTE"
    elif puntuacion >= 50:
        calidad = "BUENA"
    elif puntuacion >= 30:
        calidad = "REGULAR"
    else:
        calidad = "POBRE"

    narrativa_posible = puntuacion >= 40

    return {
        "calidad": calidad,
        "puntuacion": puntuacion,
        "problemas": problemas,
        "campos_utiles": list(set(campos_utiles)),
        "narrativa_posible": narrativa_posible,
        "total_documentos": len(documentos),
        "contexto_rico": contexto_rico
    }


def test_version_foundry_actual():
    """Test de la versiÃ³n que envÃ­a Foundry (query vacÃ­a)"""
    print("\n" + "="*60)
    print("ğŸ¤– TEST 1: VERSIÃ“N FOUNDRY ACTUAL (Query VacÃ­a)")
    print("="*60)

    payload = {"query": "", "top": 10}
    resultado = hacer_request(payload)
    analisis = analizar_calidad_respuesta(resultado)

    print(f"\nğŸ“ˆ ANÃLISIS DE CALIDAD:")
    print(f"   Calidad: {analisis['calidad']} ({analisis['puntuacion']}/100)")
    print(f"   Â¿Narrativa posible?: {analisis['narrativa_posible']}")
    print(f"   Documentos: {analisis['total_documentos']}")
    print(f"   Contexto rico: {analisis['contexto_rico']}")

    if analisis['problemas']:
        print(f"\nâŒ PROBLEMAS:")
        for problema in analisis['problemas']:
            print(f"   â€¢ {problema}")

    if analisis['campos_utiles']:
        print(f"\nâœ… CAMPOS ÃšTILES ENCONTRADOS:")
        for campo in analisis['campos_utiles']:
            print(f"   â€¢ {campo}")

    return resultado, analisis


def test_version_mejorada():
    """Test con campos que deberÃ­an mejorar la respuesta"""
    print("\n" + "="*60)
    print("ğŸš€ TEST 2: VERSIÃ“N MEJORADA (Campos Completos)")
    print("="*60)

    payload = {
        "query": "estado del wrapper y correcciones recientes",
        "agent_id": "GlobalAgent",
        "session_id": "temp_test_coherente",
        "tipo": "correccion",  # Filtrar por tipo
        "include_context": True,  # Solicitar contexto
        "include_narrative": True,  # Solicitar narrativa
        "top": 10,
        "format": "conversational"  # Formato conversacional
    }

    resultado = hacer_request(payload)
    analisis = analizar_calidad_respuesta(resultado)

    print(f"\nğŸ“ˆ ANÃLISIS DE CALIDAD:")
    print(f"   Calidad: {analisis['calidad']} ({analisis['puntuacion']}/100)")
    print(f"   Â¿Narrativa posible?: {analisis['narrativa_posible']}")
    print(f"   Documentos: {analisis['total_documentos']}")
    print(f"   Contexto rico: {analisis['contexto_rico']}")

    if analisis['problemas']:
        print(f"\nâŒ PROBLEMAS:")
        for problema in analisis['problemas']:
            print(f"   â€¢ {problema}")

    if analisis['campos_utiles']:
        print(f"\nâœ… CAMPOS ÃšTILES ENCONTRADOS:")
        for campo in analisis['campos_utiles']:
            print(f"   â€¢ {campo}")

    # Mostrar muestra de documentos
    documentos = resultado.get("response", {}).get("documentos", [])
    if documentos:
        print(f"\nğŸ“„ MUESTRA DE DOCUMENTOS RECUPERADOS:")
        for i, doc in enumerate(documentos[:3]):
            print(f"\n   ğŸ“Œ Documento {i+1}:")
            print(f"      Texto: {doc.get('texto_semantico', 'N/A')[:80]}...")
            print(
                f"      Contexto: {doc.get('contexto_conversacional', 'N/A')[:60]}...")
            print(f"      Agente: {doc.get('agent_id', 'N/A')}")
            print(f"      Timestamp: {doc.get('timestamp', 'N/A')}")
            print(f"      Tipo: {doc.get('tipo_interaccion', 'N/A')}")

    return resultado, analisis


def generar_respuesta_llm_simulada(documentos: List[Dict], metadata: Dict) -> str:
    """Simular cÃ³mo responderÃ­a un LLM con estos datos"""

    if not documentos:
        return "âŒ No se encontrÃ³ informaciÃ³n relevante en el historial."

    # Agrupar por tipo de interacciÃ³n
    correcciones = [d for d in documentos if d.get(
        'tipo_interaccion') == 'correccion']
    consultas = [d for d in documentos if d.get(
        'tipo_interaccion') == 'user_input']

    respuesta = []

    # Contexto temporal
    if metadata.get('session_id') and metadata.get('agent_id'):
        respuesta.append(
            f"ğŸ“Š **Resumen de Actividad ({metadata['agent_id']}):**")

    # AnÃ¡lisis de correcciones
    if correcciones:
        respuesta.append(
            f"\nğŸ”§ **Correcciones Realizadas ({len(correcciones)}):**")
        for i, corr in enumerate(correcciones[:3]):
            texto = corr.get('texto_semantico', '')
            timestamp = corr.get('timestamp', '')[:10] if corr.get(
                'timestamp') else 'Sin fecha'
            respuesta.append(f"   {i+1}. {texto} ({timestamp})")

    # AnÃ¡lisis de consultas recientes
    if consultas:
        respuesta.append(f"\nğŸ’¬ **Consultas Recientes ({len(consultas)}):**")
        for i, cons in enumerate(consultas[:3]):
            texto = cons.get('texto_semantico', '')
            respuesta.append(f"   â€¢ {texto}")

    # Contexto inteligente si existe
    contexto = metadata.get('contexto_inteligente', {})
    if contexto.get('resumen'):
        respuesta.append(f"\nğŸ§  **Contexto:** {contexto['resumen']}")

    return "\n".join(respuesta) if respuesta else "âŒ Datos insuficientes para generar narrativa."


def comparar_versiones():
    """Comparar ambas versiones y generar recomendaciones"""
    print("\n" + "="*80)
    print("ğŸ“Š COMPARACIÃ“N Y RECOMENDACIONES")
    print("="*80)

    # Ejecutar tests
    resultado1, analisis1 = test_version_foundry_actual()
    time.sleep(1)  # Evitar rate limiting
    resultado2, analisis2 = test_version_mejorada()

    print(f"\nğŸ“ˆ COMPARACIÃ“N DE PUNTUACIONES:")
    print(
        f"   Foundry Actual:  {analisis1['puntuacion']}/100 ({analisis1['calidad']})")
    print(
        f"   VersiÃ³n Mejorada: {analisis2['puntuacion']}/100 ({analisis2['calidad']})")

    mejora = analisis2['puntuacion'] - analisis1['puntuacion']
    print(f"   Mejora: +{mejora} puntos")

    # Simular respuestas LLM
    print(f"\nğŸ¤– SIMULACIÃ“N RESPUESTA LLM - VERSIÃ“N ACTUAL:")
    if resultado1['response'].get('documentos'):
        resp1 = generar_respuesta_llm_simulada(
            resultado1['response']['documentos'],
            resultado1['response'].get('metadata', {})
        )
        print(resp1)
    else:
        print("âŒ Sin datos para generar respuesta")

    print(f"\nğŸš€ SIMULACIÃ“N RESPUESTA LLM - VERSIÃ“N MEJORADA:")
    if resultado2['response'].get('documentos'):
        resp2 = generar_respuesta_llm_simulada(
            resultado2['response']['documentos'],
            resultado2['response'].get('metadata', {})
        )
        print(resp2)
    else:
        print("âŒ Sin datos para generar respuesta")

    # Recomendaciones
    print(f"\nğŸ’¡ RECOMENDACIONES PARA OPENAPI:")
    print("   1. Hacer 'query' opcional (no requerido)")
    print("   2. Agregar parÃ¡metros opcionales:")
    print("      â€¢ session_id: string")
    print("      â€¢ agent_id: string")
    print("      â€¢ tipo: string (enum: user_input, correccion, respuesta)")
    print("      â€¢ include_context: boolean")
    print("      â€¢ include_narrative: boolean")
    print("      â€¢ format: string (enum: raw, conversational)")
    print("   3. âš ï¸  CRÃTICO: Endpoint debe responder SIEMPRE 200")
    print("   4. Enriquecer respuesta con campos narrativos")


if __name__ == "__main__":
    print("ğŸ§ª TEST DE CAMPOS NECESARIOS PARA RESPUESTA SEMÃNTICA LLM")
    print("Comparando versiÃ³n actual vs. mejorada")

    try:
        comparar_versiones()

        print(f"\nâœ… Test completado. Revisa los resultados para optimizar el endpoint.")

    except KeyboardInterrupt:
        print(f"\nâŒ Test interrumpido por usuario")
    except Exception as e:
        print(f"\nâŒ Error en test: {e}")
